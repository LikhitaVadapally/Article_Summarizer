# Web Article Summarizer ğŸ§ 
## A simple LLM agent that:

Takes a userâ€™s topic from a Streamlit chat interface

Fetches 3 relevant articles via the Tavily API

Summarizes them into a concise, grounded answer with citations

Runs on either OpenAI models or local Ollama models (swap with one click)

Built with LangGraph, LangChain, Streamlit, and Tavily.



# ğŸ“‚ Project Structure
llm_agent/
â”œâ”€ app.py                  # Streamlit entry point (UI + workflow)

â”œâ”€ graph/
â”‚  â””â”€ agent_graph.py       # LangGraph definition (fetch â†’ summarize)

â”œâ”€ llm/
â”‚  â””â”€ model_factory.py     # Model provider factory (OpenAI vs Ollama)

â”œâ”€ retrieval/
â”‚  â””â”€ web_search.py        # Tavily API search wrapper

â”œâ”€ prompts/
â”‚  â””â”€ synthesis_prompt.txt # Instructions for summarization

â”œâ”€ requirements.txt        # Python dependencies

â”œâ”€ .env.example            # Example env vars (keys + defaults)

â””â”€ README.md               

# âš™ï¸ Installation

## 1. Clone the repo:

git clone https://github.com/your-username/llm_agent.git
cd llm_agent


## 2. Create and activate a virtual environment:

python -m venv .venv
.venv\Scripts\activate   # On Windows (PowerShell)
source .venv/bin/activate # On Mac/Linux


## 3. Install dependencies:

pip install -r requirements.txt


## 4. Set up .env:

cp .env.example .env

### Fill in:

OPENAI_API_KEY=sk-... (if using OpenAI)

TAVILY_API_KEY=tvly-...

Defaults like DEFAULT_PROVIDER=ollama

## 5. If using Ollama locally:

Install Ollama

Pull a model:

ollama pull llama3.2:3b

# â–¶ï¸ Running the App

## Start the Streamlit app:

streamlit run app.py


## Youâ€™ll see:

Local URL: http://localhost:8501
